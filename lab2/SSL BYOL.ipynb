{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIhoHTwntu1V"
   },
   "source": [
    "# Bootstrap Your Own Latent (BYOL)\n",
    "\n",
    "In this session we are going to implement Bootstrap Your Own Latent paper (https://arxiv.org/abs/2006.07733).\n",
    "\n",
    "It uses a MoCo-style training (with asymmetric SiameseNet) but with a L2 loss penalty (it is not a contrastive-base method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NVw6H3IEJVuk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0Yv5FeDvVm8"
   },
   "outputs": [],
   "source": [
    "#! scrivi come funzioan training loop di byol\n",
    "\n",
    "# loss fn\n",
    "def loss_fn(x, y):# equivalente alla distanza suller slide per byol, equivalente a cosine similarity...\n",
    "    x = F.normalize(x, dim=-1, p=2)\n",
    "    y = F.normalize(y, dim=-1, p=2)\n",
    "    return 2 - 2 * (x * y).sum(dim=-1)\n",
    "\n",
    "\n",
    "class EMA():\n",
    "    # exponential moving average\n",
    "    def __init__(self, beta):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "\n",
    "    def update_average(self, old, new):\n",
    "        if old is None:\n",
    "            return new\n",
    "        return old * self.beta + (1 - self.beta) * new# fa interpolazione alla alpha way\n",
    "\n",
    "def update_moving_average(ema_updater, ma_model, current_model):\n",
    "    for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
    "        old_weight, up_weight = ma_params.data, current_params.data\n",
    "        ma_params.data = ema_updater.update_average(old_weight, up_weight)\n",
    "\n",
    "# MLP class for projector and predictor\n",
    "\n",
    "def MLP(dim, projection_size, hidden_size=4096, sync_batchnorm=None):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim, hidden_size),\n",
    "        nn.BatchNorm1d(hidden_size),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(hidden_size, projection_size)\n",
    "    )\n",
    "\n",
    "\n",
    "class BYOL(nn.Module):# siamese net asimmetrica\n",
    "    def __init__(self, backbone, moving_average_decay = 0.99):\n",
    "        super().__init__()\n",
    "\n",
    "        self.target_ema_updater = EMA(moving_average_decay) # update with exponential moving average\n",
    "\n",
    "        self.online_net = backbone # update with SGD\n",
    "        self.online_net.fc = nn.Identity()\n",
    "        self.online_projector = MLP(512, 512, 4096)\n",
    "\n",
    "    def _get_target_encoder(self):\n",
    "        if self.target_net is None:\n",
    "            target_net = copy.deepcopy(self.online_net)\n",
    "            for p in target_net.parameters():\n",
    "                p.requires_grad = False\n",
    "            self.target_net = target_net\n",
    "        else:\n",
    "            target_net = self.target_net\n",
    "        return target_net\n",
    "\n",
    "    def update_moving_average(self):\n",
    "        update_moving_average(self.target_ema_updater, self.target_net, self.online_net)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "\n",
    "        images = torch.cat((x1, x2), dim = 0)\n",
    "\n",
    "        online_projections = self.online_projector(self.online_net(images))\n",
    "        online_pred_one, online_pred_two = online_projections.chunk(2, dim = 0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_net = self._get_target_encoder()\n",
    "\n",
    "            target_projections = target_net(images)\n",
    "            target_projections = target_projections.detach()\n",
    "            target_proj_one, target_proj_two = target_projections.chunk(2, dim = 0)\n",
    "\n",
    "        loss_one = loss_fn(online_pred_one, target_proj_two.detach())\n",
    "        loss_two = loss_fn(online_pred_two, target_proj_one.detach())\n",
    "\n",
    "        loss = loss_one + loss_two\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTq2njrMrvkM"
   },
   "source": [
    "## Exercise 0\n",
    "\n",
    "Study the above code.\n",
    "- Where is the EMA updates? # dove interviene nel training, cosa e come freezzi il gradiente?\n",
    "- Why it computes both loss_one and loss_two values? -> La loss è simmetrica, è somma di due modi diversi di dare le viste all'encoder. In Sto confrontanfdo le l'uscita della rete online su una trasformazione con l'uscita della rete target su un'altra trasformazione della stessa immagine (simil-positive)\n",
    "\n",
    "# vogliamo che le reti imparino l'una dall'altra quindi la loss deve essere effettuata tra view diverse delle 2 reti - calcolo a croce. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3lt3jwGryO-"
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "Write the training loop for moco-style training as used in BYOL.\n",
    "Use the Dataset which creates the two augmented views for each image and the Siamese Network from the past lab session [1](https://colab.research.google.com/drive/1NJwAFbRiD4MdwWf__6P2Lm0xYk_DNdVu?usp=sharing) and [2](https://colab.research.google.com/drive/1AMkh0q8L5nJScx7v6cMWoK336zqOqDY6?usp=sharing)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOk/QADKe9g1pIFWMcYznPE",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
