{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WlzRGSguPAX"
   },
   "source": [
    "# Contrastive Loss\n",
    "\n",
    "---\n",
    "In this session, we are going to implement the SimCLR loss function (https://arxiv.org/abs/2002.05709).\n",
    "\n",
    "This follows the InfoNCE loss, i.e., uses two different augmented versions of the same image as positive pair and the other images in the batch as negative samples, and the batch construction of the N-pair-mc loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "xaJjICcLhd2B"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDatasetBis(Dataset):\n",
    "    def __init__(self, data, targets=None, transform=None, target_transform=None):\n",
    "        self.imgs = data # Tensore di tutte le immagini\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgs[idx] # Sampling randomico di emlementi del dataset\n",
    "        if isinstance(img, str): # Può capitare che il dataset sia salvato come stringhe/path (da usare quando non è possibile salvarsi tutto il tensore del dataset)\n",
    "          img = read_image(img_path) # Fuzione di Torchvision, trova un'immaigne dal path fornito\n",
    "        else:\n",
    "          img = Image.fromarray(img.astype('uint8'), 'RGB')\n",
    "        label = self.targets[idx] # Non utile nel caso di self-supervised ovviamente\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img) # Utilizzo le trasformazioni\n",
    "            img2 = self.transform(img)\n",
    "            # img = self.transform(img)  Già così genero due immagini augmented diverse, siccome le funzioni che trasformano sono randomiche (TODO, rivedi le variabili)\n",
    "        if self.target_transform:\n",
    "            label1 = self.target_transform(label)\n",
    "            label2 = self.target_transform(label)\n",
    "        else:\n",
    "            label1 = label\n",
    "            label2 = label\n",
    "        return img1, img2# , label1, label2 # Concateno immaigni e labels\n",
    "    \n",
    "# simclr DA pipeline\n",
    "s=1\n",
    "size=32\n",
    "color_jitter = transforms.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n",
    "transform = transforms.Compose([transforms.RandomResizedCrop(size=size),\n",
    "                                  transforms.RandomHorizontalFlip(),\n",
    "                                  transforms.RandomApply([color_jitter], p=0.8),\n",
    "                                  transforms.RandomGrayscale(p=0.2),\n",
    "                                  transforms.GaussianBlur(kernel_size=3),\n",
    "                                  transforms.ToTensor()])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shapes: torch.Size([10, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Backbone(nn.Module):  # emulates a smaller resnet18\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))  # Outputs 256-dim vector\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (batch_size, 256)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = Backbone()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.backbone(x1)\n",
    "\n",
    "        x2 = self.backbone(x2)\n",
    "\n",
    "        x1 = F.normalize(x1, dim=1)\n",
    "        x2 = F.normalize(x2, dim=1)\n",
    "    \n",
    "        return torch.cat((x1,x2), dim=0)\n",
    "\n",
    "# Check output\n",
    "a = SiameseNet()\n",
    "input1 = torch.randn(5, 3, 32, 32)\n",
    "input2 = torch.randn(5, 3, 32, 32)\n",
    "output = a(input1, input2)\n",
    "\n",
    "print(\"Output shapes:\", output.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABbKJPlwuOb0"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# non abbiamo mai fattoo operazioni in place- io genero tutto su var di appoggio e poi il risultato lo metto su una nuova var\n",
    "#vai operazioni in place per fare boost di performance\n",
    "\n",
    "\n",
    "#bath size + grande possibile ok ma devo fare attensione a ottimizzare codice, arrivi agile a 4096x4096 e occupi moltissima memoria\n",
    "# evita di istaziare due matrici\n",
    "\n",
    "\n",
    "#poi le matrici delle maschere è molto sparso\n",
    "\n",
    "\n",
    "#1. cosa fa loss\n",
    "#2 indica dove sono positive negative ecc, aiutati con grafici x far capire cosa succede nel codice\n",
    "# no computazione o training\n",
    "import numpy as np\n",
    "class ContrastiveLossFor(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, features):\n",
    "        features = F.normalize(features, dim=1)\n",
    "        # normalize features to later compute cosine distance/similarity btw them\n",
    "\n",
    "        # compute the similarity matrix btw features\n",
    "        # (consider that feature are normalized! so the cosine similarity is ...)\n",
    "        #! similitudine tra features si fa con dot product\n",
    "\n",
    "        similarity_matrix = torch.matmul(features, features.T)\n",
    "        # print(similarity_matrix.shape) # expected 128*128\n",
    "        start = time.time()\n",
    "        \n",
    "        batch_size=features.shape[0] // 2#easy version\n",
    "        logits = torch.zeros(2*batch_size, 2*batch_size-1) # expected 128*127 -> 2N * 2N-1\n",
    "        for idx, val in enumerate(similarity_matrix):\n",
    "            # print( 'val shape:', val.shape)\n",
    "            row = torch.zeros(2*batch_size-1) # 127\n",
    "            pos_idx = idx + batch_size if idx < batch_size else idx-batch_size\n",
    "            row[0] = val[pos_idx]\n",
    "            row[1:]=torch.tensor([v for i,v in enumerate(val) if i!= idx and i!=pos_idx])\n",
    "            logits[idx]=row\n",
    "\n",
    "        logits = logits/self.temperature\n",
    "        gt = torch.zeros(logits.shape[0],dtype=torch.long)\n",
    "        loss = self.criterion(logits,gt)\n",
    "        end = time.time()\n",
    "        print(' Loss execution time: ', end - start)\n",
    "\n",
    "\n",
    "        ## TODO\n",
    "\n",
    "        # create the logits tensor where:\n",
    "        #   - in the first position there is the similarity of the positive pair\n",
    "        #   - in the other 2N-1 positions there are the similarity w negatives\n",
    "        # the shape of the tensor need to be 2Nx2N-1, with N is the batch size\n",
    "        # in diagonale ho elemento*elemento, va rimossa\n",
    "        #  devi identificare i positicw da mandare a ground truth\n",
    "        # , metti similitudine coppia positive e similitudine positive negative\n",
    "\n",
    "        # to compute the contrastive loss using the CE loss, we just need to\n",
    "        # specify where is the similarity of the positive pair in the logits tensor\n",
    "        # since we put in the first position we create a gt of all zeros\n",
    "        # N.B.: this is just one of the possible implementations!\n",
    "        return loss\n",
    "    \n",
    "class ContrastiveLossVect(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, features):\n",
    "        features = F.normalize(features, dim=1)\n",
    "        similarity_matrix = torch.matmul(features,features.T)\n",
    "\n",
    "        start = time.time()\n",
    "        labels = torch.cat([torch.arange(features.shape[0]//2) for i in range(2)], dim=0)\n",
    "        labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()\n",
    "        mask = torch.eye(labels.shape[0], dtype=torch.bool)\n",
    "        labels = labels[~mask].view(labels.shape[0],-1)\n",
    "        similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0],-1)\n",
    "        positives = similarity_matrix[labels.bool()].view(labels.shape[0],-1)\n",
    "        negatives = similarity_matrix[~labels.bool()].view(labels.shape[0],-1)\n",
    "        logits = torch.cat([positives,negatives],dim=1)\n",
    "        logits = logits / self.temperature\n",
    "\n",
    "        gt = torch.zeros(logits.shape[0],dtype=torch.long,device=logits.device)\n",
    "        end = time.time()\n",
    "        print('Loss vect execution time:', end - start)\n",
    "        return self.criterion(logits,gt)    \n",
    "    \n",
    "class ContrastiveLossOpt(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, features):\n",
    "        features = F.normalize(features, dim=1)\n",
    "        similarity_matrix = torch.matmul(features,features.T)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # TODO optimize me with in place operations\n",
    "        labels = torch.cat([torch.arange(features.shape[0]//2) for i in range(2)], dim=0)\n",
    "        labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()\n",
    "        labels = labels[~torch.eye(labels.shape[0], dtype=torch.bool)].view(labels.shape[0],-1)\n",
    "\n",
    "        similarity_matrix = similarity_matrix[~torch.eye(labels.shape[0], dtype=torch.bool)].view(similarity_matrix.shape[0],-1)\n",
    "\n",
    "        logits = torch.cat([\n",
    "            similarity_matrix[labels.bool()].view(labels.shape[0],-1), # positive\n",
    "            similarity_matrix[~labels.bool()].view(labels.shape[0],-1) # negative\n",
    "                            ],\n",
    "                            dim=1)\n",
    "        logits = logits / self.temperature\n",
    "\n",
    "        gt = torch.zeros(logits.shape[0],dtype=torch.long, device=logits.device)\n",
    "\n",
    "        end = time.time()\n",
    "        print('Loss opt execution time:', end - start)\n",
    "        return self.criterion(logits,gt)    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "torch.Size([16256])\n"
     ]
    }
   ],
   "source": [
    "data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)   \n",
    "trainset = CustomImageDatasetBis(data.data, data.targets, transform=transform)\n",
    "dataloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "x1,x2 = next(iter(dataloader))\n",
    "model = SiameseNet().to('cuda')\n",
    "x1,x2 = x1.to('cuda'), x2.to('cuda')\n",
    "features = model(x1, x2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "        # normalize features to later compute cosine distance/similarity btw them\n",
    "features = F.normalize(features, dim=1)\n",
    "        # compute the similarity matrix btw features\n",
    "        # (consider that feature are normalized! so the cosine similarity is ...)\n",
    "        #! similitudine tra features si fa con dot product\n",
    "\n",
    "similarity_matrix = torch.matmul(features, features.T)\n",
    "mask = ~torch.eye(similarity_matrix.size(0), dtype=torch.bool, device=similarity_matrix.device)\n",
    "off_diagonal_elements = similarity_matrix[mask]\n",
    "print(off_diagonal_elements.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Agvz2chP0Ndg"
   },
   "source": [
    "Let's now use the Dataset which creates the two augmented views for each image and the Siamese Network from the past lab session [1](https://colab.research.google.com/drive/1NJwAFbRiD4MdwWf__6P2Lm0xYk_DNdVu?usp=sharing) and [2](https://colab.research.google.com/drive/1AMkh0q8L5nJScx7v6cMWoK336zqOqDY6?usp=sharing) and create a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ow1YmaRPt-Py"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Loss vect execution time: 0.012261629104614258\n",
      "Loss vect execution time: 0.007040500640869141\n",
      "Loss vect execution time: 0.0055620670318603516\n",
      "Loss vect execution time: 0.007747650146484375\n"
     ]
    }
   ],
   "source": [
    "data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)   \n",
    "trainset = CustomImageDatasetBis(data.data, data.targets, transform=transform)\n",
    "dataloader = DataLoader(trainset, batch_size=64, shuffle=True, pin_memory=True)# fast computation\n",
    "\n",
    "model = SiameseNet().to('cuda')\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = ContrastiveLossOpt()\n",
    "\n",
    "for idx, (img1,img2) in enumerate(dataloader):\n",
    "    img1  = img1.to('cuda')\n",
    "    img2 = img2.to('cuda')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    features = model(img1, img2)\n",
    "    loss = criterion(features) #criterion(x1,x2, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if idx == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.0009448528289794922"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files already downloaded and verified\n",
    "\n",
    "\n",
    "Loss vect execution time: 0.00289154052734375\n",
    "\n",
    "Loss vect execution time: 0.0009870529174804688\n",
    "\n",
    "Loss vect execution time: 0.0019037723541259766\n",
    "\n",
    "Loss vect execution time: 0.0010981559753417969\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN4Jjg2VpmpXLpt/+vJuOUk",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
