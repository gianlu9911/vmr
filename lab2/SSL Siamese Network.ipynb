{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vg7BGBFjv5SP"
   },
   "source": [
    "# Siamese Network\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "In this session, we are going to implement a Siamese Network.\n",
    "\n",
    "It takes as input two augmented versions of the same image and produces as output two feature vectors one for each version of the image.\n",
    "\n",
    "For simplicity, we will use the same backbone to process the views as in SimCLR paper.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "wnoJ4nz6t3rN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torchvision.io import read_image\n",
    "\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEu_h8dhx8Lz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shapes: torch.Size([5, 10]) torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Backbone(nn.Module):  # emulates a smaller resnet18\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))  # Outputs 256-dim vector\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (batch_size, 256)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = Backbone()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.backbone(x1)\n",
    "        x1 = self.projection(x1)\n",
    "\n",
    "        x2 = self.backbone(x2)\n",
    "        x2 = self.projection(x2)\n",
    "\n",
    "        x1 = F.normalize(x1, dim=1)\n",
    "        x2 = F.normalize(x2, dim=1)\n",
    "    \n",
    "        return x1, x2\n",
    "\n",
    "# Check output\n",
    "a = SiameseNet()\n",
    "input1 = torch.randn(5, 3, 32, 32)\n",
    "input2 = torch.randn(5, 3, 32, 32)\n",
    "output1, output2 = a(input1, input2)\n",
    "\n",
    "print(\"Output shapes:\", output1.shape, output2.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Backbone(nn.Module):  # emulates a smaller resnet18\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))  # Outputs 256-dim vector\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (batch_size, 256)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AsimmetricSiameseNet(nn.Module): # TODO MAKE ME WITH DIFFERENT ENCODERS\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = Backbone()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "        self.backbone1 = Backbone()\n",
    "        self.projection1 = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.backbone(x1)\n",
    "        x1 = self.projection(x1)\n",
    "\n",
    "        x2 = self.backbone1(x2)\n",
    "        x2 = self.projection1(x2)\n",
    "\n",
    "        x1 = F.normalize(x1, dim=1)\n",
    "        x2 = F.normalize(x2, dim=1)\n",
    "    \n",
    "        return x1, x2\n",
    "\n",
    "# Check output\n",
    "a = SiameseNet()\n",
    "input1 = torch.randn(5, 3, 32, 32)\n",
    "input2 = torch.randn(5, 3, 32, 32)\n",
    "output1, output2 = a(input1, input2)\n",
    "\n",
    "print(\"Output shapes:\", output1.shape, output2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, data, targets=None, transform=None, target_transform=None):\n",
    "        self.imgs = data # Tensore di tutte le immagini\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgs[idx] # Sampling randomico di emlementi del dataset\n",
    "        if isinstance(img, str): # Può capitare che il dataset sia salvato come stringhe/path (da usare quando non è possibile salvarsi tutto il tensore del dataset)\n",
    "          img = read_image(img_path) # Fuzione di Torchvision, trova un'immaigne dal path fornito\n",
    "        else:\n",
    "          img = Image.fromarray(img.astype('uint8'), 'RGB') # Preso un array restituisce un'immagine RGB, senza non si riesce a lavorare\n",
    "        if self.targets:\n",
    "          label = self.targets[idx] # Non utile nel caso di self-supervised ovviamente\n",
    "        if self.transform:\n",
    "          img1 = self.transform(img) # Utilizzo le trasformazioni\n",
    "          img2 = self.transform(img)  # Già così genero due immagini augmented diverse, siccome le funzioni che trasformano sono randomiche (TODO, rivedi le variabili)\n",
    "        if self.target_transform:\n",
    "          label1 = self.target_transform(label)\n",
    "          label2 = self.target_transform(label)\n",
    "        return img1, img2, label # Concateno immaigni e labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlyBgKKzyxyB"
   },
   "source": [
    "Let's now use the Dataset which creates the two augmented views for each image from the [past lab session](https://colab.research.google.com/drive/1NJwAFbRiD4MdwWf__6P2Lm0xYk_DNdVu?usp=sharing) and create a loop with forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "NJAgRkIzyUTq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# simclr DA pipeline\n",
    "s=1\n",
    "size=32\n",
    "color_jitter = transforms.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n",
    "transform = transforms.Compose([transforms.RandomResizedCrop(size=size),\n",
    "                                  transforms.RandomHorizontalFlip(),\n",
    "                                  transforms.RandomApply([color_jitter], p=0.8),\n",
    "                                  transforms.RandomGrayscale(p=0.2),\n",
    "                                  transforms.GaussianBlur(kernel_size=3),\n",
    "                                  transforms.ToTensor()])\n",
    "\n",
    "data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n",
    "# create training set from CustomDataset\n",
    "trainset = CustomImageDataset(data.data, data.targets, transform=transform)\n",
    "dataloader = DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "model = SiameseNet()\n",
    "for idx, data in enumerate(dataloader):\n",
    "    views1, views2, targets = data\n",
    "    print(views1.shape)\n",
    "    print(views2.shape)\n",
    "    print(targets.shape)\n",
    "\n",
    "    output = model(views1, views2)\n",
    "\n",
    "    if idx == 3:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMRVywWas5MaA3coPhE3E60",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
